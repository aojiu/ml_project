{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_news1 = pd.read_csv(\"articles1.csv\")\n",
    "raw_news2 = pd.read_csv(\"articles2.csv\")\n",
    "raw_news3 = pd.read_csv(\"articles3.csv\")\n",
    "\n",
    "print(raw_news1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_date_expression(date):\n",
    "    if date[4] == \"-\":\n",
    "        return date\n",
    "    \n",
    "    break_point = []\n",
    "    for i in range(len(date)):\n",
    "        if date[i] == '/':\n",
    "            break_point.append(i)\n",
    "    \n",
    "    if len(break_point) != 0:\n",
    "        out = date[0:break_point[0]] + '-' + date[break_point[0]+1:break_point[1]]+'-'+date[break_point[1]+1:]\n",
    "        return out\n",
    "    else:\n",
    "        return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_day(dataset,dict_count):\n",
    "    for i in range(dataset.shape[0]):\n",
    "        date = dataset[i][5];\n",
    "        if dict_count.__contains__(date):\n",
    "            dict_count[date] += 1\n",
    "        else:\n",
    "            dict_count[date] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_caculation(dic):\n",
    "    relavences = [\"social_media\",\"computer\",\"business\",\"programming\",\"hearing\",\"urban\",\"shopping\",\"science\",\"work\",\"valuable\",\"fashion\",\"technology\",\"competing\",\"economics\",\"office\"]\n",
    "    negetive_attitude = ['hate','aggression','horror','suffering','ridicule','irritability','deception','disappointment','negative_emotion','nervousness']\n",
    "    positive_attitude = ['cheerfulness','optimism','celebration','trust','positive_emotion']\n",
    "    \n",
    "    # correlation c:\n",
    "    c = 0\n",
    "    for relavence in relavences:\n",
    "        c += dic[relavence]\n",
    "    c = float(c) / float(len(relavences))\n",
    "    \n",
    "    # negetive attitude:\n",
    "    neg_att = 0\n",
    "    for neg in negetive_attitude:\n",
    "        neg_att += dic[neg]\n",
    "    neg_att = float(neg_att) / float(len(negetive_attitude))\n",
    "    \n",
    "    # positive attitude:\n",
    "    pos_att = 0\n",
    "    for pos in positive_attitude:\n",
    "        pos_att += dic[pos]\n",
    "    pos_att = float(neg_att) / float(len(positive_attitude))\n",
    "    \n",
    "    return float(c*100), float(c*neg_att*1000), float(c*pos_att*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_dataset(dataset):\n",
    "    # first we need to drop and add some columns\n",
    "    # the colums we need to drop is \"id\", \"title\", \"publication\", \"author\", \"year\", \"month\", \"url\", \"Unnamed: 0\"\n",
    "    dataset = dataset.drop(columns = [\"id\", \"title\", \"publication\", \"author\", \"year\", \"month\", \"url\", \"Unnamed: 0\"])\n",
    "    dataset[\"correlation\"] = 0\n",
    "    dataset[\"positive_relavent\"] = 0\n",
    "    dataset[\"negetive_relavent\"] = 0\n",
    "    \n",
    "    dataset[\"correlation\"] = dataset.correlation.astype(float)\n",
    "    dataset[\"positive_relavent\"] = dataset.positive_relavent.astype(float)\n",
    "    dataset[\"negetive_relavent\"] = dataset.negetive_relavent.astype(float)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_analysis_with_only_three_value(dataset):\n",
    "    # the colums we need to add is sentiment analysis result \n",
    "    # plus three special colums, which are correlation c, positive attitude p * c, negetive attitude n *c.\n",
    "    # second we need to change the all the date style into 2017-1-1\n",
    "    # all this need to do in a loop:\n",
    "    for i in range(dataset.shape[0]):\n",
    "        # change date:\n",
    "        if pd.isna(dataset[\"date\"][i]) == True:\n",
    "            print(dataset[\"date\"][i])\n",
    "        else:\n",
    "            dataset[\"date\"][i] = change_date_expression(dataset[\"date\"][i])\n",
    "        \n",
    "        # analysis the contont:\n",
    "        ana_dict = lexicon.analyze(dataset[\"content\"][i], normalize = True)\n",
    "        #for key, value in ana_dict.items():\n",
    "            #dataset[key][i] = value\n",
    "        # puls three special values.\n",
    "        if ana_dict is None:\n",
    "            dataset[\"correlation\"][i] = 0\n",
    "            dataset[\"positive_relavent\"][i] = 0\n",
    "            dataset[\"negetive_relavent\"][i] = 0\n",
    "        else:\n",
    "            c, neg, pos = relevance_caculation(ana_dict)\n",
    "            dataset[\"correlation\"][i] = c\n",
    "            dataset[\"positive_relavent\"][i] = neg\n",
    "            dataset[\"negetive_relavent\"][i] = pos\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print(dataset[\"date\"][i])\n",
    "        \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_put_result(dataframe, n):\n",
    "    file_name = str(n)+\"_part_anaylsis.csv\"\n",
    "    \n",
    "    dataframe1 = pre_processing_dataset(dataframe)\n",
    "    dataframe1_with_analysis = processing_analysis(dataframe1)\n",
    "    dataframe1_with_analysis = dataframe1_with_analysis.drop(columns = [\"content\"])\n",
    "    \n",
    "    dataframe1_with_analysis.to_csv(file_name, encoding='utf-8')\n",
    "    \n",
    "    return dataframe1_with_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"social_media\",\"computer\",\"business\",\"programming\",\"hearing\",\"urban\",\"shopping\",\"science\",\"work\",\n",
    "#      \"valuable\",\"fashion\",\"technology\",\"competing\",\"economics\",\"office\"]\n",
    "# ['hate','aggression','horror','suffering','ridicule','irritability','deception',\n",
    "#      'disappointment','negative_emotion','nervousness']\n",
    "# ['cheerfulness','optimism','celebration','trust','positive_emotion']\n",
    "# now what i should do is keep all this values\n",
    "# so need dataframe with columns: \"date\", \"\"social_media\",\"computer\",\"business\",\"programming\",\"hearing\",\n",
    "#       \"urban\",\"shopping\",\"science\",\"work\",\n",
    "#      \"valuable\",\"fashion\",\"technology\",\"competing\",\"economics\",\"office\", 'hate','aggression',\n",
    "#       'horror','suffering','ridicule','irritability','deception',\n",
    "#      'disappointment','negative_emotion','nervousness', 'cheerfulness','optimism','celebration','trust',\n",
    "#      'positive_emotion',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_analysis_with_all_variables(dataset):\n",
    "    # pre-processing the dataset for droping useless colums\n",
    "    dataset = dataset.drop(columns = [\"id\", \"title\", \"publication\", \"author\", \"year\", \"month\", \"url\", \"Unnamed: 0\"])\n",
    "    # we need to have a list hold all the parameters\n",
    "    check_list = [\"social_media\",\"computer\",\"business\",\"programming\",\"hearing\",\"urban\",\"shopping\",\"science\",\"work\",\"valuable\",\"fashion\",\"technology\",\"competing\",\"economics\",\"office\",'cheerfulness','optimism','celebration','trust','positive_emotion','hate','aggression','horror','suffering','ridicule','irritability','deception', 'disappointment','negative_emotion','nervousness']    \n",
    "    # inital matrix that have all the columens:\n",
    "    data_list = [\"date-month-day\"]\n",
    "    for parameter in check_list:\n",
    "        data_list.append(np.float16(0))\n",
    "    \n",
    "    for i in range(3):\n",
    "        data_list.append(np.float16(0))\n",
    "        \n",
    "    data_matrix = np.array([data_list])\n",
    "    \n",
    "    print(data_matrix)\n",
    "    \n",
    "    for i in range(dataset.shape[0]):\n",
    "        # change date:\n",
    "        if pd.isna(dataset[\"date\"][i]) == True:\n",
    "            print(dataset[\"date\"][i])\n",
    "        else:\n",
    "            dataset[\"date\"][i] = change_date_expression(dataset[\"date\"][i])\n",
    "        \n",
    "        # analysis the contont:\n",
    "        ana_dict = lexicon.analyze(dataset[\"content\"][i], normalize = True)\n",
    "        #for key, value in ana_dict.items():\n",
    "            #dataset[key][i] = value\n",
    "        # puls three special values.\n",
    "        \n",
    "        if ana_dict is not None:\n",
    "            one_line_list = []\n",
    "            one_line_list.append(dataset[\"date\"][i])\n",
    "            for parameter in check_list:\n",
    "                one_line_list.append(ana_dict[parameter])\n",
    "            relev, pos, neg = relevance_caculation(ana_dict)\n",
    "            one_line_list.append(relev)\n",
    "            one_line_list.append(pos)\n",
    "            one_line_list.append(neg)\n",
    "            \n",
    "            data_matrix = np.insert(data_matrix, 0, [one_line_list], 0)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print(dataset[\"date\"][i])\n",
    "    \n",
    "    dataframe = pd.DataFrame(data_matrix, columns=['date', \"social_media\",\"computer\",\"business\",\"programming\",\"hearing\",\"urban\",\"shopping\",\"science\",\"work\",\"valuable\",\"fashion\",\"technology\",\"competing\",\"economics\",\"office\",'cheerfulness','optimism','celebration','trust','positive_emotion','hate','aggression','horror','suffering','ridicule','irritability','deception', 'disappointment','negative_emotion','nervousness' ,'correlation', 'positive_relavent','negetive_relavent'])\n",
    "\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_result_into_csv(dataframe, n):\n",
    "    file_name = str(n)+\"_part_anaylsis_with_all_parameters.csv\"\n",
    "    \n",
    "    dataframe1_with_analysis = processing_analysis_with_all_variables(dataframe)\n",
    "    \n",
    "    dataframe1_with_analysis.to_csv(file_name, encoding='utf-8')\n",
    "    \n",
    "    return dataframe1_with_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = get_final_result_into_csv(raw_news1,1)\n",
    "\n",
    "news1.head(10)\n",
    "\n",
    "news2 = get_final_result_into_csv(raw_news2,2)\n",
    "\n",
    "news2.head(10)\n",
    "\n",
    "news3 = get_final_result_into_csv(raw_news3,3)\n",
    "\n",
    "news3.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
